s# 딥러닝의 역사

딥러닝은 인공 신경망(Artificial Neural Network)에 뿌리를 두고 있으며, 여러 번의 침체기와 부흥기를 거쳐 현재의 모습에 이르게 되었습니다.

## 1. 딥러닝의 태동기 (1940년대 ~ 1960년대)
*   **1943년:** 워런 매컬러와 월터 피츠가 처음으로 뇌의 뉴런을 간소화된 이진 출력을 갖는 전기 회로로 모델링했습니다. 이는 인공 뉴런의 개념적인 시작이었습니다.
*   **1958년:** 프랑크 로젠블랫이 퍼셉트론(Perceptron)을 발명했습니다. 퍼셉트론은 입력 데이터에 가중치를 곱하고 이를 합산하여 임계값과 비교하는 간단한 분류 모델로, 초기 형태의 신경망 학습 알고리즘이었습니다.
*   **1969년:** 마빈 민스키와 시모어 페퍼트가 저서 '퍼셉트론'에서 단층 퍼셉트론이 XOR과 같은 간단한 비선형 문제를 해결할 수 없다는 것을 수학적으로 증명하면서, 인공지능 연구에 첫 번째 암흑기(AI Winter)가 찾아옵니다.

## 2. 제2의 부흥과 침체기 (1980년대 ~ 1990년대)
*   **1980년대:** 다층 퍼셉트론(Multi-Layer Perceptron, MLP)과 역전파(Backpropagation) 알고리즘이 주목받기 시작했습니다. 역전파는 신경망의 가중치를 효율적으로 업데이트하는 방법을 제시하여 다층 구조의 신경망 학습을 가능하게 했습니다.
*   **1980년:** 후쿠시마 쿠니히코가 시각 피질의 구조를 모방한 네오코그니트론(Neocognitron)을 발표했습니다. 이는 오늘날의 합성곱 신경망(CNN)의 원형으로 평가받습니다.
*   **1990년대:** 얀 르쿤(Yann LeCun)이 역전파 알고리즘을 적용한 합성곱 신경망(CNN)인 LeNet-5를 개발하여 손글씨 숫자 인식에서 높은 성능을 보였습니다.
*   **1990년대 후반:** 하지만 신경망의 층이 깊어질수록 학습이 어려워지는 문제(기울기 소실 문제, Vanishing Gradient Problem)가 대두되면서 다시 한번 연구가 침체기를 맞습니다.

## 3. 딥러닝의 혁명 (2000년대 중반 ~ 현재)
*   **2006년:** 제프리 힌튼 교수가 '제한된 볼츠만 머신(RBM)'을 이용한 사전 학습(Pre-training) 방법으로 깊은 신경망(Deep Belief Network, DBN)의 기울기 소실 문제를 해결하는 돌파구를 마련했습니다. 이 시점부터 '딥러닝'이라는 용어가 널리 사용되기 시작했습니다.
*   **2012년:** 이미지넷(ImageNet) 이미지 인식 대회(ILSVRC)에서 알렉스 크리제브스키가 제프리 힌튼과 함께 개발한 '알렉스넷(AlexNet)'이 압도적인 성능으로 우승을 차지했습니다. 이는 GPU를 활용한 병렬 연산의 힘과 깊은 CNN의 가능성을 전 세계에 증명한 결정적인 사건이었습니다.
*   **2014년 이후:** ResNet, GAN(생성적 적대 신경망), Transformer 등 혁신적인 모델들이 등장했습니다. 특히 Transformer는 자연어 처리(NLP) 분야에 혁명을 가져왔고, 이는 GPT와 같은 거대 언어 모델(LLM)의 기반이 되었습니다.
*   **현재:** 딥러닝은 컴퓨터 비전, 자연어 처리, 음성 인식 등 거의 모든 인공지능 분야에서 핵심 기술로 자리 잡았으며, 그 영향력을 계속해서 확장하고 있습니다.
